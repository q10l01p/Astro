---
title: Extreme Parkour with Legged Robots
description: 论文笔记
publishDate: 2025-03-24T16:39
---
<img src="https://imgbed.hijie.me/file/paper/Extreme Parkour with Legged Robots/1742811035315_Extreme Parkour with Legged Robots.png" alt="Extreme Parkour with Legged Robots.png" width=100%>

# 引言

## **研究背景与挑战**

1. **人类跑酷的启发**

   * 跑酷（Parkour）是一种动态障碍跨越运动，要求运动员通过精密的视觉-肌肉协调，在高风险场景（如长跳、高跳、斜坡奔跑）中实现精准动作。
   * 人类通过长期实践学习，仅依赖生物传感器（如视觉）和肌肉控制，无需精确建模环境，即可适应复杂地形。
2. **机器人跑酷的局限性**

   * **传统方法**：依赖高精度传感（如地形映射）、显式规划（如动作路径优化）和严格控制，需提前获知障碍物参数（如尺寸、位置）。此类方法对环境变化（如障碍移动或未知场景）鲁棒性差。
   * **硬件限制**：低成本机器人（如Unitree A1）因传感器噪声（深度相机低频、延迟）、执行器精度不足，难以支持传统方法依赖的复杂映射和规划。

## **研究动机**

* **类人学习范式**：模仿人类通过端到端试错学习（强化学习），直接从原始传感器输入（单目深度图像）输出动作，无需中间映射或显式规划。
* **技术目标**：在低成本硬件上实现“极端跑酷”能力，包括跨越障碍物高度达机器人身高**2倍**、跨越间隙长度达身长**2倍**，以及倒立行走等高难度动作。

## **本文核心观点**

1. **问题重新定义**

   * **动态方向调整**：传统方法依赖人工指定航向指令，但在复杂障碍场景（如斜坡急转弯）中难以实时提供精准指令。需让机器人自主调整方向。
   * **多行为统一控制**：不同跑酷动作（跳跃、平衡、急停）差异显著，单一策略需能自适应切换行为。
2. **解决思路**

   * **双蒸馏方法（Dual Distillation）**：分阶段训练策略：

     * **阶段1**：使用特权信息（如地形扫描点scandots和航向指令）在仿真中预训练策略。
     * **阶段2**：通过蒸馏移除特权信息，使策略直接从深度图像预测航向和动作。
   * **统一奖励设计**：基于内积的方向跟踪奖励、边缘安全惩罚、风格化动作引导（如手倒立时强制朝下姿态），实现多任务端到端优化。

## **关键贡献**

* 提出端到端框架，克服低成本机器人传感与执行器的不精确性，实现极端跑酷行为。
* 首次在Unitree A1上实现跨越障碍高度和距离达机器人尺寸**2倍**的性能，远超传统方法（如并发工作仅1.5倍）。
* 验证了无需显式地形映射或人工航向指令的可行性，展示泛化至未知地形（如柔软草地、楼梯）的能力。

# 相关工作

## **1. 四足机器人运动控制（Legged Locomotion）**

* **经典方法**：依赖地形映射（如点云融合）和基于模型的控制（如动态步态规划）。

  * 优点：在已知地形或结构化环境中表现稳定。
  * 局限性：对传感器噪声敏感，难以泛化到未知或动态障碍场景。
  * 典型工作：Anymal系列机器人（如Hutter et al.）依赖高精度地形地图和预定义路径规划。
* **强化学习与视觉驱动方法**：

  * 近年研究转向端到端强化学习（RL），直接从视觉输入（如深度图像）输出动作，减少对精确传感器的依赖。
  * 代表成果：

    * Agarwal et al.（2022）实现了鲁棒的视觉导航，但未涉及高难度跑酷动作。
    * Margolis et al.（2022）通过盲式强化学习（Blind RL）实现动态奔跑，但依赖简化环境。
  * 局限性：现有方法在跨越高难度障碍（如2倍身高的障碍物）或复杂方向调整时表现不足。

- - -

## **2. 机器人跑酷（Robotic Parkour）**

* **传统方法**：

  * Rudin et al.（2022）结合地形映射和RL，但依赖预先定义的障碍参数，无法适应未知环境。
  * 其他早期工作（如BD-Parkour）依赖显式规划，需提前测量障碍尺寸，限制实用性。
* **近期进展**：

  * Margolis et al.（2022）和Yu et al.（2023）通过强化学习实现了敏捷跳跃和奔跑，但通常在无复杂障碍或简化环境中测试。
  * Cassie系列机器人（Li et al.）展示了跳跃能力，但依赖复杂硬件（如Cassie）而非低成本平台。
* **关键挑战**：

  * 高精度动作（如长跳、高跳）对传感器和执行器的延迟/噪声敏感，传统方法难以鲁棒执行。
  * 需要自主调整方向（如跨越斜坡时急转弯），而现有方法多依赖人工指定航向。

- - -

## **3. 并发工作（Concurrent Work）**

* **Hoeller et al.（2023）**（AnymalC机器人）：

  * 使用模块化策略（任务特定子策略+高阶控制器），但依赖地形映射和简化障碍抽象（如类型、尺寸）。
  * 局限性：无法处理复杂几何障碍，且硬件成本高（AnymalC为工业级机器人）。
  * 性能：跳跃高度为机器人身高的**1.1-2倍**，但动作幅度小于本文。
* **Zhuang et al.（2023）**（Unitree A1）：

  * 端到端策略，但依赖简化地形抽象（如障碍类型、距离）作为额外输入，限制泛化性。
  * 使用复杂训练流程（先模拟软约束，再蒸馏为硬约束），而本文方法更简单。
  * 性能：最高跳跃高度和跨越距离仅为本文的**75%**（1.5倍身高 vs. 本文的2倍）。
* **本文优势对比**：

  * **硬件适配性**：在低成本Unitree A1上实现更高难度动作（2倍身高/长度障碍）。
  * **输入简化**：仅依赖单目深度图像，无需地形抽象或人工航向指令。
  * **方法创新**：双蒸馏框架和统一奖励设计，支持多行为（跳跃/平衡/方向自适应）的端到端学习。

- - -

## **核心区别与贡献**

| **对比维度**  | **传统方法/并发工作**         | **本文方法**                 |
| --------- | --------------------- | ------------------------ |
| **传感器依赖** | 高精度地形映射、简化抽象信息（如障碍尺寸） | 单目深度图像，无额外传感器或人工标注       |
| **动作幅度**  | 跳跃高度/距离 ≤ 机器人身材的1.5倍  | **2倍身高跳跃、2倍身长跨距**（突破性提升） |
| **方向控制**  | 依赖人工航向指令或模块化控制器       | 策略自主预测航向，适应动态障碍          |
| **训练复杂度** | 需多阶段蒸馏或任务模块组合         | 统一奖励设计+双蒸馏，流程简化          |
| **泛化能力**  | 依赖预定义障碍参数，泛化性有限       | 在未知复杂地形（如倾斜斜坡、柔软草地）鲁棒    |

- - -

### **总结**

本文通过**双蒸馏方法**和**统一奖励设计**，解决了低成本机器人在极端跑酷任务中的核心挑战：

* 在动态障碍场景中实现**自主方向调整**，无需人工指令。
* 通过端到端学习，直接从单目深度图像输出动作，克服传感器噪声和执行器延迟。
* 性能显著超越近期并发工作（如Zhuang et al.），在动作幅度和泛化性上达到新高度，填补了低成本机器人在极限运动领域的空白。

# 方法总结

<img src="https://imgbed.hijie.me/file/paper/Extreme Parkour with Legged Robots/1742816200562_method_compressed.png" alt="method_compressed.png" width=100% />

## **核心框架：端到端双蒸馏策略**

本文提出一种**端到端双蒸馏框架**，分为两个阶段，逐步将依赖特权信息的复杂策略转化为仅依赖单目深度图像的轻量级策略。

### **第一阶段：预训练（Privileged Information + 模拟仿真）**

* **输入与输出**：

  * **输入**：单目深度图像（raw observation）**+** 特权信息（privilege）：

    * **地形扫描点（Scandots）**：通过激光雷达生成的地形点云，提供局部障碍物高度、距离等信息（仅用于预训练阶段）。
    * **人工航向指令（Heading Command）**：通过**运动规划器**生成的局部路径方向（仅用于预训练阶段）。
  * **输出**：关节动作指令（joint torques）和自主预测的航向指令（predicted heading）。
* **训练目标**：

  * 使用强化学习（PPO算法）最大化**复合奖励函数**，学习在复杂地形中执行高难度动作（跳跃、平衡）。
  * 同时，策略需通过预测航向指令，逐步减少对人工指令的依赖。

### **第二阶段：蒸馏（移除特权信息）**

* **蒸馏目标**：

  * 移除特权信息（scandots和人工航向指令），使策略仅依赖单目深度图像。
  * 通过**知识蒸馏**技术，将第一阶段策略的隐含知识（如地形理解、方向调整）压缩到轻量级策略中。
* **蒸馏过程**：

  1. **策略冻结**：固定第一阶段策略权重，作为教师网络。
  2. **学生网络训练**：在相同仿真环境中，学生网络（轻量级）通过模仿教师网络的动作和航向预测，逐步优化。
  3. **蒸馏损失函数**：结合动作指令的均方误差（MSE）和航向指令的交叉熵损失，确保学生策略继承教师能力。

### **统一奖励设计（Key Innovation）**

奖励函数设计是方法的核心，通过**多任务联合优化**实现复杂行为的端到端学习：

| **奖励组件**    | **作用**                              | **数学表达式**                                                                 |
| ----------- | ----------------------------------- | ------------------------------------------------------------------------- |
| **方向跟踪奖励**  | 奖励策略沿预设路径方向（或自主预测方向）前进，基于向量内积计算。    | $R*{\text{heading}} = \cos(\theta*{\text{curr}}, \theta_{\text{target}})$ |
| **边缘安全惩罚**  | 处罚接近障碍物边缘（如跳跃时脚部距离障碍边缘过近）以防止跌落。     | $R_{\text{safe}} = -\sum \frac{1}{d_i}$（当距离 $d_i < \text{threshold}$）     |
| **能量效率奖励**  | 鼓励动作能耗最小化，避免过度发力（如关节扭矩过大）。          | $R_{\text{energy}} = - \lVert \tau \rVert^2$                              |
| **风格化动作引导** | 强制特定行为（如手倒立时强制四足朝下）或复杂动作（如长跳时腿部姿态）。 | $R_{\text{style}} = \text{sim}(\text{action}, \text{style_template})$     |
| **存活奖励**    | 每步存活给予固定奖励，鼓励长期稳定运动。                | $R_{\text{survive}} = +1$                                                 |

### **视觉输入处理**

* **单目深度图像预处理**：

  * 使用轻量级CNN（如ResNet-18）提取图像特征，输出特征向量。
  * 特征向量与特权信息（预训练阶段）或自主预测的航向指令（蒸馏后阶段）拼接，作为策略网络输入。
* **噪声鲁棒性设计**：

  * 在仿真中注入传感器噪声（如深度图像随机模糊、关节位置测量误差）和执行器延迟，提升真实场景泛化能力。

### **动态方向调整机制**

* **自主航向预测**：

  * 策略网络的隐藏层输出航向指令（heading vector），通过航向跟踪奖励（(R_{\text{heading}})）强化方向控制能力。
  * 在复杂地形（如斜坡急转弯）中，策略通过视觉输入自主判断转向时机与幅度，无需人工指令。
* **多行为统一控制**：

  * 通过**统一动作空间**设计，策略在不同场景中自主切换行为模式：

    * **跳跃模式**：当检测到障碍物时，调整腿部发力模式。
    * **平衡模式**：在不平地面或空中翻转时，激活姿态控制模块。
    * **急停/转向模式**：根据路径动态调整关节扭矩方向。

### **训练与部署流程**

1. **仿真环境**：

   * 使用MuJoCo/PyBullet构建多样化场景（随机障碍高度、间隙距离、地面材质）。
   * 包含极端案例（如2倍身高障碍、柔软草地、动态障碍）。
2. **训练阶段**：

   * **第一阶段**：在仿真中使用特权信息预训练策略，训练轮次约100万步。
   * **第二阶段**：通过蒸馏移除特权信息，微调策略至收敛。
   * **真实世界微调**：在真实Unitree A1上进行少量实机微调（约5000步），适配硬件差异。
3. **部署**：

   * 策略仅依赖单目深度相机输入，输出实时动作指令，延迟低于50ms。

### **方法创新与优势**

1. **双蒸馏框架**：

   * **第一阶段**利用特权信息快速学习复杂行为，**第二阶段**通过蒸馏消除依赖，解决了端到端方法在模拟中难以直接学习极端动作的问题。
   * 相比传统方法，无需显式地形映射或人工航向指令。
2. **统一奖励设计**：

   * 将方向控制、安全约束、动作风格等多目标**端到端融合**，避免模块化方法的复杂性。
   * 通过风格化奖励，强制策略学习人类跑酷中的关键动作模式（如腾空姿态）。
3. **动态方向调整**：

   * 自主预测航向指令的能力，使机器人在复杂障碍场景中灵活应对突发方向变化（如斜坡急转弯）。
4. **低成本硬件适配性**：

   * 通过仿真中的噪声注入和轻量级CNN设计，策略在Unitree A1等低成本机器人上实现高难度动作，硬件计算需求低（仅需嵌入式GPU）。

### **核心贡献总结**

本文提出了一种**端到端双蒸馏框架**，通过特权信息预训练和蒸馏移除，解决了低成本四足机器人在极端跑酷场景中的三大挑战：

1. **传感器噪声与硬件限制**：仅依赖单目深度图像，无需高精度传感器。
2. **自主决策能力**：引入动态航向预测机制，消除对人工指令的依赖。
3. **多任务统一优化**：通过奖励函数设计，实现跳跃、平衡、方向调整的端到端学习。

该方法在Unitree A1上首次实现了2倍身高/跨距的跑酷动作，显著超越现有方法，为机器人在未知复杂环境中的敏捷运动提供了新范式。
