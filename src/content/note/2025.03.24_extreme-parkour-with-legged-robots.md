---
title: Extreme Parkour with Legged Robots
description: 论文笔记
publishDate: 2025-03-24T16:39
---
<div style="text-align: center;">
<img src="https://imgbed.hijie.me/file/paper/Extreme Parkour with Legged Robots/1742811035315_Extreme Parkour with Legged Robots.png" alt="Extreme Parkour with Legged Robots.png" width=75%>
</div>

# 背景介绍
足式机器人的运动控制一直是机器人领域的研究热点。近年来，随着深度强化学习的发展，足式机器人在复杂地形上的行走和奔跑能力得到了显著提升。然而，让机器人像人类一样进行跑酷（Parkour）运动仍然是一个巨大的挑战。跑酷需要机器人具备高度的动态性、精确的感知-运动协调能力，以及在极端情况下（如高跳、远跳）的极限运动能力。传统的机器人跑酷方法通常依赖于精确的环境建模、运动规划和控制，这限制了它们在非结构化环境中的应用。

# 研究目的

本论文旨在探索一种基于数据驱动的端到端学习方法，使低成本、感知和执行器都不够精确的四足机器人能够实现极限跑酷。具体目标是：

1.  仅使用一个前置的、低频、有抖动和噪声的深度相机作为感知输入。
2.  训练一个单一的神经网络策略，直接从相机图像输出电机控制指令。
3.  实现一系列高难度的跑酷动作，包括高跳（障碍物高度为机器人身高的2倍）、远跳（跨越距离为机器人身长的2倍）、倒立行走（仅用前腿行走）和在倾斜坡道上行走。
4.  在未知的、具有不同物理特性的障碍物环境中进行泛化。

# 创新点

1.  **双重蒸馏方法**：为了使机器人能够在部署时根据障碍物类型自主调整朝向，提出了一种新颖的双重蒸馏方法。首先，在阶段1使用特权朝向信息（来自预设路径点）训练策略；然后，在阶段2将该策略蒸馏为一个能够从深度图像中预测自身朝向的策略。
2.  **统一的奖励设计原则**：为了让单一神经网络能够学习多种不同的跑酷技能，提出了一个基于内积的通用奖励设计原则。该原则能够自动适应不同的地形几何形状，并产生多样化的运动行为。
3.  **自动地形课程学习**：为了解决强化学习中的探索难题，设计了一个自动地形课程，让机器人在训练过程中逐步挑战更难的地形。
4.  **极限跑酷性能**：在低成本的Unitree A1机器人上实现了前所未有的跑酷性能，包括2倍身高的高跳、2倍身长的远跳、倒立行走和在倾斜坡道上的跳跃。

# 方法论

<div style="text-align: center;">
  <img src="https://imgbed.hijie.me/file/paper/Extreme Parkour with Legged Robots/1742816200562_method_compressed.png" alt="method_compressed.png" width="50%">
</div>

本论文采用了基于深度强化学习的端到端训练框架。主要步骤包括：

1.  **统一奖励函数设计**：
    *   **速度跟踪奖励**：使用机器人实际速度与期望速度以及期望方向（由路径点计算）之间的内积来鼓励机器人朝着目标方向移动。
    *   **脚部间隙惩罚**：惩罚机器人在靠近地形边缘的地方落脚，以提高在真实环境中的稳定性。
    *   **风格化奖励**：通过跟踪期望的前向向量（同样使用内积设计），鼓励机器人探索不同的运动风格（如倒立行走）。
    *   **正则化项**：使用已有的正则化项来提高运动的平滑性和稳定性。

2.  **阶段1：基于Scandots的强化学习**：
    *   使用上述奖励函数在模拟环境中训练一个策略网络。
    *   输入包括本体感受信息（关节角度、角速度等）、Scandots（一种特权环境信息，类似于激光雷达点云）、目标朝向、行走标志和期望速度。
    *   使用正则化在线适应（ROA）方法训练一个适应模块来估计环境属性。
    *   采用自动地形课程，根据机器人的表现动态调整训练地形的难度。

3.  **阶段2：蒸馏方向和外感受信息**：
    *   使用监督学习（DAgger算法）将阶段1的策略蒸馏为一个可部署的策略。
    *   将Scandots输入替换为一个卷积神经网络-GRU（ConvNet-GRU）结构，用于处理深度图像。
    *   使用行为克隆（Behavior Cloning）来训练方向预测网络，预测阶段1中使用的预设路径点提供的目标朝向。
    *   为了避免数据分布漂移，提出了一种教师-学生混合（MTS）方法，在训练过程中结合使用预测的朝向和真实的朝向。

# 关键细节

*   **Scandots**：作为一种特权信息，Scandots提供了比原始深度图像更结构化的环境表示，有助于策略的学习。
*   **ROA**：通过在线估计环境属性，ROA使得策略能够适应不同的环境变化。
*   **MTS**：通过混合使用预测和真实的朝向，MTS有效地缓解了数据分布漂移问题，提高了方向预测的准确性。
*   **实验设置**：使用Unitree A1机器人和Intel RealSense D435深度相机。策略网络和深度图像处理都在Jetson NX上运行。

# 实验结果

1.  **仿真结果**：
    *   在四种不同的地形（倾斜坡道、台阶、间隙和障碍物）上进行了测试。
    *   提出的方法在平均X轴位移（MXD）和平均边缘违规（MEV）两个指标上均优于基线方法。
    *   消融实验验证了内积奖励、脚部间隙惩罚和双重蒸馏方法的有效性。

2.  **真实世界结果**：
    *   在真实环境中进行了测试，包括高跳、远跳、倒立行走和在倾斜坡道上的跳跃。
    *   提出的方法在所有环境中均取得了比基线方法更高的成功率。
    *   特别是在倾斜坡道上，由于提出的方法能够自主调整朝向，因此表现显著优于依赖人工指定朝向的基线方法。
    *   展示了机器人能够实现2倍身高的高跳和2倍身长的远跳。

# 结论

本论文提出了一种基于数据驱动的端到端学习方法，成功地使低成本、感知和执行器都不够精确的四足机器人实现了极限跑酷。通过双重蒸馏方法、统一的奖励设计原则和自动地形课程学习，机器人能够仅使用一个前置深度相机就实现高难度的跑酷动作，并在未知的障碍物环境中进行泛化。这项工作为低成本机器人在复杂环境中的自主运动控制开辟了新的可能性。

# 未来展望

*   将该方法扩展到移动机械臂领域，实现更复杂的移动操作任务。
*   进一步探索更通用的奖励函数设计，以实现更多样化的运动行为。
*   研究如何将视觉信息与其他传感器（如IMU）相结合，以提高机器人的感知能力和鲁棒性。
*   探索在真实世界中进行在线学习的可能性，使机器人能够不断适应新的环境和任务。
